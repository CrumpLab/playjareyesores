1 chapter 1 big data science: a philosophy of science perspective brian d. haig university of canterbury introduction the advent of big data and its place in science has recently received considerable attention in both science studies and popular literatures. and, in the last two or three years, articles on big data analysis have started to appear in psychology’s methodological literature (e.g., harlow & oswald, 2016; mcabee, landis, & burke, 2017; oswald & putka, 2016; tonidandel, king, & cortina, 2018). in this chapter, i am not concerned with big data as such (i.e., with the collection, curation, and analysis of large data sets), but with big data science, or, as it is sometimes called, data-intensive science. dealing with data is of major importance to science, of course, but my focus is on the nature of science more broadly construed. big data science is sometimes claimed to be a new paradigm that provides us with a revolutionary conception of scientific methodology (hey, tansley, & tolle, 2009): one that analyses very large data sets for correlations rather than causes, employs inductive rather than hypotheticodeductive method, and eschews theories that makes reference to causal mechanisms. consideration of methodological matters such as these in big data science center on metascientific concepts that are the stock-in-trade of philosophers of science. in recent years, philosophy of science has increasingly sought to understand science as it is practiced, and it is now well positioned to help us understand the nature and status of science. my purpose in this chapter is to make use of relevant philosophical literatures and undertake a critical conceptual examination of a number of central claims made by methodologists about big data science. copyrighted material; not for further distribution 2 i begin by considering which philosophy of science is best for examining big data science. i maintain that both global and local formulations of the philosophy of scientific realism are most appropriate. i then evaluate the provocative, and related, anti-realist claims that the advent of big data science spells the end of both scientific method and theory as we standardly understand them. in doing so, i identify the different roles of inductive and abductive methods in an altered understanding of both big data and small data science, and go on to identify different types of theory and their relevance for both conceptions of science. i also ask whether or not it is viable for big data science to place an emphasis on correlational research at the expense of causal explanatory research. i suggest that big data science needs to accommodate a number of conceptions of causality. many of the challenges facing big data science are challenges faced by science more generally. accordingly, the chapter emphasizes the commonalities between big data science and other forms of science. it also proceeds on the assumption that the philosophical work on big data biology has relevance for psychological science. thus far, most of the emerging research on the philosophy of big data science is to be found in the philosophy of biology. in this respect, i will have particular regard for the work of sabina leonelli (2016), emanuele ratti (2015), and wolfgang pietsch (2015, 2016, 2018). there are instructive lessons for psychology in works such as these, apart from the fact that parts of psychology can reasonably be construed as biological science (bunge, 1990). at the end, i summarize the chapter, and broach the topic of scientific realism again, suggesting that future work on the philosophy of big data science should draw from this philosophy. realist philosophy for big data science in this chapter, i adopt a scientific realist perspective on science, including big data copyrighted material; not for further distribution 3 science. although it is the subject of considerable debate, and is opposed by many anti-realist philosophies, scientific realism is the dominant philosophy of science today. with its current heavy emphasis on the nature of scientific practice, it has become a valuable repository for understanding science – it is a philosophy for science, not just a philosophy of science. in light of considerable efforts to meet anti-realist challenges, one can reasonably claim that scientific realism harbors our best philosophical accounts of science. scientific realism, like many “isms”, comes in a variety of forms and, therefore, cannot be characterized in a straightforward manner. for my purposes, two basic forms of realism need to be distinguished, both of which can help us improve our understanding of big data science. these are global realism and local realism. global realism comprises a family of doctrines and provides a reservoir of resources that can be drawn from in order to help fashion local realisms. local realisms are formulated contextually in order to cast light on real differences in subject matters. global realism most formulations of realism are global in nature (e.g., psillos, 1999), in the sense that they are depicted as overarching general philosophies of science. more often than not, three core doctrines are presented in expositions of global realism: (1) metaphysical realism, which maintains that there is a real world of which we are part, and which science investigates; (2) semantic realism, which claims that the referring expressions of scientific theories should be taken at face value; and (3) epistemological realism, which asserts that both observable and unobservable features of the world can be known by the use of scientific methods. sometimes, an additional thesis, (4) axiological realism, is added. this is a thesis about the aims of science, and it standardly says that science primarily aims for true theories. finally, the family of realist copyrighted material; not for further distribution 4 theses is occasionally extended to include (5) institutional realism. this thesis maintains that science is a human social endeavor and is subject to institutional as well as theoretical determinations. because of its importance to realism, something should be said about realist methodology, a topic that can be regarded as a core feature of the thesis of epistemological realism just noted. i believe that realism provides us with a particularly rich conception of methodology, which is of considerable help in understanding and guiding research. the resourcefulness of realist methodology is hinted at in the following brief adumbration of its major characteristics (haig, 2014; hooker, 1987; nickles, 1987). first, the methodology has three major tasks: it describes how methods function; it critically evaluates methods against their rivals; and, it recommends how to pursue chosen research goals. second, realist methodology is critically aim-oriented. it recommends the pursuit of valuable truth, explanatory understanding, and effective control; and, it is concerned with the mutual adjustment of appropriate means and desired ends. third, realist methodology is a substantive domain that uses the methods of the various sciences to study method itself; in this sense it is naturalistic. a fourth feature of the methodology is that it is both generative and consequentialist. generative methodology involves reasoning to, and accepting, knowledge claims in question from warranted premises. consequentialist methodology focuses on reasoning from knowledge claims in question to their testable consequences. fifth, realist methodology acknowledges the need for both reliabilist and coherentist approaches to justifying knowledge claims. with reliabilism, a belief is justified to the extent that it is acquired by reliable processes. with coherentism, a belief is justified in virtue of its coherence with other beliefs. these different forms of justification are complementary and of equal importance. as a sixth feature, realist methodology regards science as a problem-oriented endeavor in which problems are copyrighted material; not for further distribution 5 conceptualized as constraints on their effective solution. the constraints are constitutive of the problem itself and serve to give it structure. they variously include heuristics, rules, and principles. finally, realist methodology takes the researcher’s makeup as a “knowing subject” seriously. amongst other things, the researcher is regarded as a satisficer (simon, 1956) who makes heavy use of heuristics to guide her inquiries. realist methodology undergirds a wide variety of methods, strategies, and heuristics that have been successfully used to produce worthwhile knowledge in the various sciences. local realism as a global philosophy of science, fashioned in an image of physics, scientific realism presumably is intended to apply to all sciences at all times. an important consequence of this outlook is that global realism is of limited value as a philosophy for the behavioral and social sciences, which have generally been less successful than the natural sciences in their theoretical achievements. in order to take advantage of the understanding of science that realism is capable of providing, the behavioral sciences need local, fine-grained formulations of realism that are appropriate to their particular natures and achievements (haig, 2014; kincaid, 2000; mäki, 2005). one productive way to proceed would be to replace the main theses of global realism with revised theses along the lines suggested by uskali mäki (2005). what follows is a gloss on five core realist theses, three of which are influenced by mäki’s formulations. 1. possible existence. the thesis of epistemological realism maintains that our best theories entitle us to believe in the existence of the hidden entities they postulate. however, all sciences exhibit uneven rates of theoretical progress and, therefore, different degrees of epistemic confidence should attach to the different phases of the development and appraisal of scientific copyrighted material; not for further distribution 6 hypotheses and theories. when a scientist first postulates a new entity, it is more appropriate to hold that the entity might exist, rather than maintain that it does exist, and that we give ourselves sufficient time to show that does exist. considerable progress is required before one can express confidence in its existence. 2. mind dependence. the thesis of metaphysical realism customarily insists on the mindindependence of the world. although this is appropriate for the physical sciences, it is inappropriate for the large tracts of non-neuroscientific behavioral and social science. mental and social objects, such as beliefs and money, are mind-dependent in the sense that they are partly constituted by our conceptions or representations of them. money is a familiar example of an ontologically subjective entity. something is money only because we regard it as money. however, the objectivity required in studying such entities is safeguarded by insisting that they are inquiryindependent, even though they are mind-dependent. theories of mental and social objects typically do not have the power to create those objects, and this is sufficient to satisfy the demand that such objects be studied independently. 3. possible truth. the above remarks about existence apply in analogous fashion to truth. orthodox realism says that our best theories in the mature sciences are literally true, or approximately true, and that the appropriate use of reliable methods enables us to say that this is the case. however, rather than take our best theories to be true, or approximately so, a realism that is sensitive to the growth of scientific knowledge should accept the view that our theories might well be true in the future, if not right now. this will certainly be the case when our theories are first conceived. therefore, it is more realistic to nominate our theories as candidates for truth. consistent with this, truth should be understood as an orienting ideal, which we approximate by fashioning and justifying our theories. because we cannot expect immediate truth in science, truth copyrighted material; not for further distribution 7 should be understood as a distal goal, not a proximal goal. 4. observables and unobservables. standard formulations of realism claim that theoretical entities exist, and that science’s best theories successfully refer to such entities. however, mäki (2005) thinks that the social sciences, including folk psychology, mostly study observed or manifest entities, which he calls commonsensibles. these include familiar objects such as money, stock markets, beliefs and attitudes, and social institutions. for mäki, these sorts of entities are part of our familiar observed ontology. i agree with him that some of our commonsensibles are observables. however, i think that many of them (e.g., folk psychological properties) have the status of unobserved theoretical entities. it is important to let the ontological chips fall where they may. 5. aims. we have seen that axiological realism depicts science as an aim-oriented endeavor concerned with discovering truths about the world. however, science is complex and varied, and for this reason it is better thought of as pursuing multiple aims. in addition to pursuing truth, science is also concerned with achieving understanding through the establishment of facts and theories, as well as the attainment of control, which is broadly understood to include, for example, the experimental regulation of inquiry, and the application of knowledge to bring about desirable social outcomes. all that said, i believe that there is good reason to embrace both global and local realism. construed globally, realism is a general, systematic theory (hooker, 1987) that combines the full range of relevant theses in a coherent manner. for this reason, the ability of realism to help make best sense of the varied and complex nature of science is maximized by appealing to our best formulation of general realism, not local realism. however, i think that full-blown realism will do much of its day-to-day work as a reservoir of theses from which one can selectively draw as an aid copyrighted material; not for further distribution 8 in formulating local realisms as specific occasions demand. this broad-brush run-through of the primary features of global realism, and one promising formulation of local realism, might seem like an unnecessary excursion. however, i want to signal the fact that the methodological matters i deal with are best understood against a backdrop of realist philosophy of science. i will have occasion to note some explicit links between realism and big data science. scientific method and big data science modern science is a complex ensemble of different parts. it simultaneously pursues aims, employs methods, fashions theories, and is embedded in institutions, but scientific method is undoubtedly its centerpiece. the centrality of method to science stems from the fact that it provides scientists with the primary form of guidance in their quest to obtain knowledge about the world. much of our understanding of the nature and place of methods in science comes in the form of theories of scientific method. undoubtedly, the most prominent of these are the hypothetico-deductive and inductive accounts of method. much less known, but just as important for science, is a third account of method, known as the abductive theory of method (curd, 1980; haig, 2014). inductive method has often been associated with narrow forms of empiricist thinking that urge us to stay close to the facts. the hypothetico-deductive method has been used to test hypotheses and theories, and fits comfortably within the more liberal philosophy of logical empiricism. it has been, and continues to be, the method of choice for most scientists. the abductive method, it should be emphasized, is tailor-made for realist theorizing about hidden entities. given my focus on big data science, i will largely restrict my attention to the inductive and abductive accounts of method. however, i believe that all three theories of method can be copyrighted material; not for further distribution 9 productively employed in a methodologically pluralist conception of science. big data science is often presented as having revisionist implications for our understanding of scientific inquiry, and commentaries about these implications have focused on the first two of these three theories of scientific method. for example, in his well-known pronouncements about big data science, chris anderson (2008) declares, “the scientific method is based around testable hypotheses. … but faced with massive data, this approach to science – hypothesis, model, test – is becoming obsolete”. in effect, this is to suggest that we should abandon the hypothetico-deductive method as we know it. the most common recommendation is that big data science should employ an inductive conception of scientific method in its place. in renouncing the tacit realist philosophy of most working scientists, some advocates of inductive inquiry in big data science speak of a new era of empiricism in which big data analytic techniques allow the data to speak for themselves in a theory-free manner (kitchin, 2014). further, among big data scientists with realist preferences, one rarely encounters the suggestion that big data science needs an abductive conception of scientific method in which data-driven research leads beyond data patterns and empirical generalizations to explanatory hypotheses about the patterns of interest extracted from the data. clearly, what is said about big data and scientific method deserves our critical scrutiny. in considering the place of inductive and abductive accounts of scientific method in big data science, i will endeavor to provide a fuller explication of the methods than big data proponents have typically provided, and say what realist goals they serve in the production of scientific knowledge. inductive method and big data science in the absence of an accompanying elaboration and justification, anderson’s assertion copyrighted material; not for further distribution 10 that big data science is simply inductive has little more than shock value. one has to look elsewhere for a decent treatment of the topic. in philosophy, chalmers (2013) offers an accessible and informative account of inductive method in which the scientist is portrayed as reasoning inductively by enumeration from secure observation statements about singular events to laws or theories in accordance with some governing principle of inductive reasoning. sound inductive reasoning is held to create and justify theories simultaneously, so that there is no need for subsequent empirical testing. some have criticized this view of method for placing too much trust in the powers of observation and inductive generalization, and for believing that enumerative induction is all there is to scientific inference. in modern behavioral science, the radical behaviorism of b. f. skinner is a prominent example of a research tradition that uses an inductive conception of scientific method (skinner, 1984). within this behaviorist tradition, the purpose of research is to detect empirical phenomena of learning that are subsequently systematized by non-explanatory theories. radical behaviorism is an empiricist philosophy, but because it subscribes to an instrumentalist conception of theories as non-explanatory organizational devices, it is not dustbowl empiricism. for this latter form of empiricism is typically characterized as being decidedly atheoretical. psychologists mcabee, landis, and burke (2017) provide an assessment of the value of big data science for organizational psychology, and recommend a measured inductive perspective for such research in order to counter the hegemony of hypotheticodeductive theorizing. although the inductive method has received considerable criticism, especially from those who seek to promote hypothetico-deductive and abductive conceptions of scientific inquiry, it nevertheless stresses, in a broad-brush way, the scientific importance of fashioning empirical copyrighted material; not for further distribution 11 generalizations. in the next section, i outline an abductive theory of scientific method, which employs a methodologically detailed account of inductive method in the form of enumerative induction (“induction by generalization”) in order to detect empirical phenomena as precursors to the abductive construction of theories. a final point to be made here is that enumerative inductive methods are not the only type of inductive method employed in science. in the later section dealing with causation and big data science, i draw attention to wolfgang pietsch’s use of eliminative inductive methods to establish causal claims. abductive method and big data science it is well known that science makes use of both inductive and deductive forms of inference. less well known is the fact that science also makes considerable use of a third type of reasoning known as abductive inference, a form of inference that it sometimes confused with inductive inference. briefly, abductive inference is explanatory inference, and in science it involves reasoning about hypotheses, models, and theories in a manner that explains the relevant facts (haig, 2005, 2014; magnani, 2001). there are different species of abductive reasoning having to do with the generation, development, and appraisal of hypotheses and theories, and i will come to these shortly. given that my focus is on scientific method and big data science, it should be emphasized at the outset that different forms of scientific reasoning should not be equated with different accounts of scientific method. theories of scientific method are structuring devices that involve the efficient, systematic ordering of inquiry. as such, they describe ordered sequences of actions that comprise strategies designed to achieve research goals that have to do with the construction of knowledge (nickles, 1987; haig, 2014). to reach their goals, theories of scientific method copyrighted material; not for further distribution 12 employ forms of inference and, as we will see, individual theories of method will often involve more than one form of inference. i turn now to consider the abductive theory of scientific method, which is heavily informed by elements of the realist methodology listed earlier. here, i follow my own formulation of the method (haig, 2005; 2014). according to the abductive theory of method, scientific inquiry proceeds as follows: guided by evolving research problems that comprise packages of empirical, conceptual, and methodological constraints, sets of data are analyzed in order to detect robust empirical regularities, or phenomena. once detected, these phenomena are explained by abductively inferring the existence of underlying causes responsible for their production. upon positive judgments of the initial plausibility of the explanatory theories about these causes, attempts are made to elaborate on the nature of the causal mechanisms in question. this is done by constructing plausible models of those mechanisms by analogy to relevant ideas in domains that are already well understood. when the theories are well developed, they are assessed against their rivals with respect to their explanatory goodness. this assessment involves making judgments of the best of competing explanations. phenomena detection. to establish that data are reliable evidence for the existence of phenomena, scientists use a variety of strategies. a statistically-oriented, multi-stage model of data analysis provides one characterization of phenomena detection. the model proceeds through the four stages of initial data analysis, exploratory data analysis, close replication, and constructive replication. these four stages are concerned respectively with data quality, pattern suggestion, pattern confirmation, and empirical generalization. the process of phenomena detection is one of enumerative induction in which one learns empirically, on a case-by-case basis, the conditions of applicability of the empirical generalizations that represent the copyrighted material; not for further distribution 13 phenomena. the account of phenomena detection glossed here is consistent with a realist outlook on science. this outlook commits one to the view that phenomena are ontological existents of various kinds, including empirical generalizations. as previously noted, many of the phenomena in areas of psychology such as psychophysics and neuropsychology are parts of the world’s furniture that exist independently of human interests, theoretical commitments, and sociocultural factors. other phenomena, to be found in such areas as social and economic psychology, do not exist independently of these social factors. however, one can still retain a realist outlook on phenomena that are influenced, and/or constituted, by such factors. the local realist thesis of mind-dependence described earlier safeguards scientific objectivity by regarding these social entities as inquiry-independent, even though they are mind-dependent. another realist methodological feature of phenomena detection is its use of a reliabilist approach to justification. recall, reliabilism maintains that a knowledge claim is justified to the extent that it is acquired by reliable processes or methods. for example, the data-analytic stages of close and constructive replication just mentioned are, in effect, consistency tests that are employed to help show on reliabilist grounds that that phenomena exist. theory construction. for the abductive theory of method, theory construction comprises three methodological phases: theory generation, theory development, and theory appraisal, with the first two phases being temporal in nature. the abductive theory characterizes all three phases of theory construction as abductive in nature, although the character of abductive inference is different in each case. theory generation in the abductive theory of method takes place through a process of existential abduction in which the existence, but not the nature, of causal mechanisms is copyrighted material; not for further distribution 14 hypothesized in order to explain the production of empirical phenomena. exploratory factor analysis is an example of a method that facilitates the existential abductive generation of theories, or hypotheses about latent factors that are thought to underlie patterns of correlations in manifest variables (haig, 2014). the abductive logic of exploratory factor analysis enables the method to confer a generative justification on the theories it produces. this form of justification, which is a realist methodological commitment, involves judgments that the theories are the result of sound abductive reasoning in the sense that they have sufficient initial plausibility to warrant further investigation. generative justification warrants claims for possible truth, not literal truth, which is a feature of the local formulation of realism given earlier. the abductive theory of method is also a method for theories-in-the-making. it encourages researchers to look upon their theories as developing entities. because we often do not have knowledge of the nature of the causal mechanisms we abductively probe, such nascent theories stand in clear need of further specification. the abductive method urges us to construct models of those mechanisms by imagining something analogous to mechanisms whose nature we do know. because analogical modeling increases the content of explanatory theories, the reasoning it embodies is referred to as analogical abduction. with analogical modeling, one builds an analogical model of the unknown subject or causal mechanism based on the known nature and behavior of the source from which the model is drawn. the local realist idea of possible truth extends to the abductive phase of theory development. judgments of the initial plausibility of claims about the causal entities in the phase of theory generation are strengthened by further judgments of the plausibility of the analogical models, but they await a final appraisal in terms of the process of inference to the best copyrighted material; not for further distribution 15 explanation, to which i now turn. the abductive theory of method takes the systematic evaluation of mature theories to be an abductive undertaking known as inference to the best explanation, whereby a theory is accepted when it is judged to provide a better explanation of the evidence than its rivals. in particular, it takes inference to the best explanation to be centrally concerned with establishing explanatory coherence (thagard, 1992). the theory of explanatory coherence maintains that the propositions of a theory hold together because of their explanatory relations. the determination of the explanatory coherence of a theory is made in terms of three criteria: explanatory breadth, simplicity, and analogy. explanatory breadth is the most important criterion for choosing the best explanation. it captures the idea that a theory is more explanatorily powerful than its rivals if it explains a greater range of facts. the notion of simplicity deemed most appropriate for theory choice is captured by the idea that preference should be given to theories that make fewer special or ad hoc assumptions. analogy is considered an important criterion of inference to the best explanation because explanations are judged more coherent if they are supported by analogy to theories that scientists already find credible. relations of explanatory coherence are established through the operation of a number of principles and a computer program (see thagard, 1992, for details). true to its name, the theory of explanatory coherence adopts a coherentist approach to justification, which contrasts with the reliabilist approach to justification employed when justifying claims about empirical phenomena. as noted above, coherence justification too is a realist methodological principle. i believe that the abductive theory of scientific method has the methodological resources to help us understand and evaluate a good part of the structure of big data inquiry. there are a copyrighted material; not for further distribution 16 number of points to be made in this regard: 1. the advocates of big data science are correct in claiming that not all scientific inquiry needs to conform to the stricture of hypothetico-deductive hypothesis testing, though, as will be mentioned later, the hypothetico-method may form part of a more expansive view of dataintensive science. the abductive theory of method is quite different from the hypotheticodeductive method, for it promotes a data-before-theory sequence of inquiry, seeks to generate explanatory theories by methodological means, and appraises theories in terms of their explanatory goodness, not in terms of their predictive success. 2. the abductive theory of method provides a detailed methodological account of the discovery of empirical phenomena, typically in the form of empirical generalizations. when advocates of big data inquiry speak favorably about adopting an inductive conception of inquiry, they say little about the nature of the nature of inductive reasoning involved. by contrast, the abductive theory gives a detailed methodological account of the process of enumerative induction involved in phenomena detection, and the statistical methods and scientific strategies that can be employed in so doing. the role of eliminative inductive inference in an alternative conception of big data inquiry is yet to be considered. 3. although most proponents of big data science speak in favor of inductive research, they occasionally suggest that data-intensive science should be thought of as abductive in nature (e.g., fox & hendler, 2014) but they do not elaborate on the claim (johns, jamieson, and jones, 2018, is an exception). the abductive theory of method is so-called because it emphasizes the importance of abduction in science, conceives the theory construction process as abductive through and through, and provides a detailed characterization of a number of abductive methods and strategies that can be employed to further that theory construction process. copyrighted material; not for further distribution 17 4. phenomena detection is a relatively autonomous part of scientific inquiry, and it is an important scientific goal in its own right. but it also provides a natural stimulus for theory construction, which is equally important in science. big data scientists who want to go no further than a concern with data analytics have given no convincing reason for eliminating, or downplaying, the importance of theory construction in science. 5. rob kitchen (2014) has suggested that “[d]ata-driven science … is more open to using a hybrid combination of abductive, inductive and deductive approaches to advance the understanding of a phenomenon.” (p. 5), though his sequencing of these elements is difficult to discern. the abductive theory of method advocates a combination of inductive and abductive methods, where the sequence is, in specific terms: enumerative induction (phenomena detection), existential abduction (theory generation), analogical induction (theory development), inference to the best explanation (comparative theory appraisal). note, additionally, that the abductive theory of method can be profitably regarded as a broad framework theory in which more specific research methods can be employed to give the parent theory its operational bite. to conclude this section, i want to suggest that it is a merit of the abductive theory of scientific method that it clearly distinguishes between inductive and abductive inference and assigns a different research goal to each. some advocates of inductive theory building mix up the two. edwin locke (2015), for example, characterizes induction as moving from the particular to the general yet, at the same time, seems to understand it as taking us to mediating causal mechanisms. however, inductive inference is descriptive inference, countenancing more entities of the same kind, whereas it is abductive inference that enables us to reason about entities that are different in kind from those to be found in their evidential base (haig, 2014). the place of theory in big data science copyrighted material; not for further distribution 18 the end of theory? as was noted earlier in this chapter, anderson (2008) maintains that the advent of big data science renders the very idea of theory obsolete. he makes this claim in the following forthright manner: out with every theory of human behavior, from linguistics to sociology. forget taxonomy, ontology, and psychology. who knows why people do what they do? the point is that they do it, and we can track and measure it with unprecedented fidelity. with enough data, the numbers speak for themselves. (p.3) i shall use this quotation as a stepping-off point to examine the extent to which, and in what forms, theory has a role in a credible view of big data science. i begin by rejecting anderson’s claim that the end of theory is nigh. i then outline and discuss sabina leonelli’s novel view that classificatory theories play an indispensable role in data-intensive science. finally, i suggest that scientific theories have no uniform nature or canonical structure, there being a variety of different types of theory serving different purposes in science. anderson’s proclamation that the advent of big data spells the end of theory as we know it in science is highly implausible, and can be dealt with quickly. the first, and obvious, point to make is that observations (data, facts) are theory-laden, a general conclusion reached in the middle of the last century by prominent philosophers of science such as n. r. hanson, thomas kuhn, and paul feyerabend. the theory-ladenness arises unavoidably from various sources, such as human interests, research questions, scientific methods, and theoretical assumptions. mindful that there are different types of theory-ladenness, it is also useful to distinguish between being theory-informed, and being theory-driven. leonelli (2016) uses this distinction to argue that datacentric biology is theory-informed, but not theory-driven, in the sense that it is not concerned copyrighted material; not for further distribution 19 with the testing of theories but with the formation of theories that arise out of data-analytic work. pietsch (2015) similarly argues that frequently used algorithms, such as classificatory trees and nonparametric regression, are theory-laden, but not in a way that begs questions about the causal structure of the phenomena being examined. it is worth pointing out here that in his haste to dismiss theory altogether, anderson fails to consider the possibility that an instrumentalist conception of scientific theory that does not invoke causal explanations, and that is constructed after the facts are in, might well suit his strong empiricist preferences. as noted in the previous section, b. f. skinner (1984) did precisely this, while adopting an inductive conception of scientific method. classificatory theories for big data science as part of her extensive philosophical examination of data-centric biology, leonelli (2016) argues for the importance of classificatory theories in the field. although data-centric biology is not coextensive with big data biology, what leonelli says about classificatory theories has a direct bearing on how one might think about theory in big data science. rejecting the naïve idea that data-intensive science is a theory-free inductive process, she suggests that data curators in fact perform the important role of constructing classificatory theories in order to enable data to “travel” between scientists within, and across, disciplines. for this to happen, “ontologies”, or labelling devices (e.g., the ‘gene ontology’), based on biological entities and processes used in research, are created to provide the stability needed to search, retrieve, and transport data from data bases. although leonelli sometimes speaks of classificatory theories as networks of ordered terms, she really intends them to be understood as networks of interconnected propositions that refer to biological entities and processes. the descriptive sentences contained in the biocopyrighted material; not for further distribution 20 ontologies have the same epistemic function as testable hypotheses. classificatory theories themselves may be likened to “bottom-theory” that emerges from the practices of data handling. in this sense, they do not involve theoretical terms that refer to new entities. classificatory theories, then, differ from other, more familiar, forms of scientific theory, such as those that center on law-like propositions, and those that focus on claims about causal mechanisms. nonetheless, they have a number of features that philosophers of science identify as the good-making features of theories, even though these features are understood somewhat differently. leonelli (2016) identifies and discusses four important features of classificatory theories: generality, unification, explanation, and synthesizing frameworks. classificatory theories aim for generality in the form of restricted generalizations that apply locally to narrowly-defined domains. they also have unifying power, which is reductive rather than universal, in that they seek commonality among phenomena without embedding them in an overarching conceptual structure. further, classificatory theories are explanatory in the sense that they answer “how” questions without appeal to law-like statements, or mechanisms. however, this criterion for a good classificatory theory is said by leonelli to be a secondary epistemic virtue, with precedence being given to virtues such as empirical accuracy, breadth of understanding, and heuristic value. however, i think that these three just-mentioned virtues are, in fact, often held to be highly desirable properties of an explanatory theory. finally, classificatory theories provide perspectives for guiding research that are methodological rather than conceptual; that is, they identify the methodological commitments of specific research programs, rather than broad visions for biological knowledge. to conclude this brief consideration of classificatory theories, we can say that their presence in data-intensive biology adds to our understanding of theories beyond the more copyrighted material; not for further distribution 21 familiar explanatory conceptions of theories discussed by philosophers of science. they also help us to appreciate just how unrealistic it is to portray data-centric and, therefore, big-data science as theory-free inductive undertakings. finally, the role played by classificatory theories in science is the result of extending the “new experimentalism” in the philosophy of science to a concern with data curation. before that extension, theory was primarily seen as an explanatory undertaking that only came after a focus on the role that experimental data played as evidence for the establishment of empirical phenomena. theories are networks with no canonical structure heavily influenced by cronbach and meehl’s (1955) ground-breaking paper on construct validity, psychology remains in thrall to the idea that scientific theories are nomological networks – that is, networks of propositions containing laws, with implicit definitions that coordinate observational and theoretical terms. however, this conception of theory, a product of the now outdated mid-twentieth-century logical empiricist philosophy of science, is ill-suited to psychology. two reasons for this are that most of psychology does not have genuine laws, and that it does not use implicit definitions to coordinate observational and theoretical terms in the network. more realistically, scientific theories can be conceived as networks of claims about empirical phenomena, models of causal mechanisms, and coherence relations between propositions in the theories. this is the conception of theories that results from using the abductive theory of scientific method. this, too, is a network view of theories, but it is not the nomological network view. its components are those we find in much scientific theorizing today. of course, other network formulations of theories are possible, such as those depicted in structural equation models and complex network theory. it is important to appreciate that copyrighted material; not for further distribution 22 leonelli’s classificatory view of theories is explicitly understood by her as a network conception of theories. cronbach and meehl, and the logical empiricists, were wrong to think that scientific theories could be given a canonical formulation as nomological nets. a more realistic portrayal points to the need for local conceptions of theories that are tailored to a discipline’s various natures and achievements, whether that discipline be psychology or not. causation, correlation, and big data science the idea that modern science frequently makes causal inferences based on correlational data is anathema to some big data science enthusiasts. well known in this regard is anderson’s (2008) provocative, and cryptic, contention that “correlation supersedes causation, and science can advance even without coherent models, unified theories, or really mechanistic explanation at all”. more recently, and in like manner, viktor mayer-schönberger and kenneth cukier (2013) argued that big data science involves a “move away from the age-old search for causality” to correlation. the claim that correlation supersedes causation will strike many scientists as wrongheaded, for the fairly obvious reason that knowledge of causes is a major means by which we come to explain empirical phenomena, and increase our understanding of them. further, knowledge of relevant causes is often found to be useful in applying the findings of science. when delving into the extensive literature on the nature of causation and its place in science, one is struck by the variety of accounts on offer; there is nothing like a consensus on how we should characterize this important idea. however, rather see this variety as a conflicting diversity, it would be better to think of these accounts as an “amiable jumble” that sustains a realistic attitude of causal pluralism, whereby different accounts of causation are suited to different purposes (godfrey-smith, 2009). in what follows, i consider three different accounts of copyrighted material; not for further distribution 23 causation, all of which i believe are relevant for understanding big data science. these are the regularity, difference-making, and mechanistic theories of causation. causes as regularities the idea that correlation supersedes causation was strongly expressed by karl pearson, the author of the product-moment correlation, dearly beloved by psychologists. with a strong interest in the philosophy of science, and influenced in particular by the british empiricist philosophers of his day, pearson (1911) rejected, as metaphysically unacceptable, the idea that causes produce their effects. instead, he embraced the idea that association, or correlation, was all that there was; functional relations were to be taken as the mark of a mature science. anderson was quick to dismiss causation, but one might imagine that pearson’s attitude to causation, and his successor concept of correlation, fit quite well with anderson’s stated rejection of causation in big data science. although psychologists make heavy use of pearson’s product-moment correlation, they invariably go beyond its strictures and explicitly engage in causal reasoning. more often than not, their causal inferences are governed by what is known as the regularity theory of causation (harré & madden, 1975). true to its name, this theory maintains that a causal relation is an actual, or hypothetical, regularity between different events. more specifically, a relationship between two variables x and y can properly count as causal only when three conditions obtain: (1) x precedes y in time; (2) x and y covary; and (3) no additional factors enter into, and confound, the x-y relationship (e.g., kenny, 1979). enshrined in its textbooks, the regularity theory might well be considered psychology’s official view of causation. interestingly, in presenting their considered conception of big data science for organizational science, mcalbee, landis, and burke (2017) appear to subscribe to this regularity view of causation. copyrighted material; not for further distribution 24 it is worth pointing out that the third condition of this regularity theory imposes an odd restriction on the causal thinking of its users (haig, 2003). for, in ruling out alternative causal interpretations to a direct causal relationship between x and y, it is simultaneously maintained that the cause, x, in the x-y relationship must be a direct and, presumably, observed cause. yet in establishing this requirement, the existence of confounding third variables in causal fields must be considered a genuine possibility. oddly, these, often unobserved, causes are taken to be causally relevant, but only for the purpose of discounting their existence! the oddity here lies in the fact that implementing the regularity theory of causation requires the researcher to provisionally step outside its bounds and momentarily adopt a more liberal generative theory of causation (harré & madden, 1975), where the third variables are considered as potential productive forces. despite this oddity, something like the regularity theory of causation might well appeal to those big data scientists who do not want to go beyond the confines of empirical regularities and fathom the workings of underlying causal mechanisms. causes as difference-makers the second conception of causation expands on the idea that causes are things that make a difference. happily, one formulation of this account has been explicitly formulated with dataintensive science in mind. in a series of papers, wolfgang pietsch (2015, 2016, 2018) has developed a philosophically rigorous conception of data-intensive science that subscribes to a difference-making account of causation, and an eliminative-inductive conception of scientific method that together help produce useful scientific knowledge about empirical phenomena. central to a number of different approaches to causation is the idea that a cause is something that makes a difference to its effects, although there are different accounts of how this idea should be unpacked. one prominent account explicates difference-making in terms of copyrighted material; not for further distribution 25 claims about counterfactual relationships. on this view, a cause makes a difference to an effect in the sense that if the cause had occurred, so would the effects; and if it had not occurred, the effects would not have occurred either. traditional counterfactual approaches to causation determine the truth-value of counterfactuals in terms of the rarefied notion of possible world semantics, which is far removed from the practice of actual science. by contrast, pietsch’s (2016) difference-making account of causation is determined by the scientific practice of using the inductive method of difference, or its twin, the direct method of agreement. of relevance for psychological researchers here is the fact that j. s. mill’s joint method of agreement and difference lies behind fisherian analysis of variance. in his own words, pietsch brings the scientific method of eliminative induction into play as follows: the best known and arguably most effective method is the so-called method of difference that establishes causal relevance of a boundary condition cx by comparing two instances which differ only in cx and agree in all other circumstances c. if in one instance, both cx and a are present and in the other both cx and a are absent, then cx is causally relevant to a. there is a twin method to the method of difference, called the strict method of agreement, which establishes causal irrelevance, if the change in cx has no influence on a. (p.8) two important points about this gloss on pietsch’s difference-making account of causation should be made here. first, his account is based on eliminative induction, which focuses on boundary conditions, and it does not lead to a regularity view of causation. the regularity theory, by contrast, is founded on enumerative induction, which is concerned with establishing the number of confirming/disconfirming instances. second, pietsch’s account of causation is concerned with the causal structure of claims about empirical phenomena, and thereby serves a copyrighted material; not for further distribution 26 conception of data-intensive science that is concerned with making predictive inferences without appeal to knowledge of underlying causal processes. as i remark in the concluding section, pietsch’s conception of data-intensive science is a new form of empiricism. causes as mechanisms a third, and important, approach to causation focuses not on difference-making, but on tracing the causal processes involved in the production of effects. we saw earlier that anderson’s presentation of big data science eschewed a concern with causal mechanisms. and we have just seen that pietsch is clear that his focus on difference-making excludes appeal to such processes. however, i believe that one can adopt a defensible perspective on data-intensive science that does concern itself with causal processes. this possibility brings us naturally to mechanistic theories of causation. today, mechanistic theories of causation occupy a prominent place in the philosophical literature on causation. glennan (1996) succinctly characterizes the mechanistic approach by declaring that “a relation between two events (other than fundamental physical events) is causal when and only when these events are connected in the appropriate way by a mechanism”. (p 56) as such, the mechanistic account goes well beyond the strictures of the regularity account of causation; the mechanisms and their components are, by their very nature, productive of change. about the idea of a mechanism itself, bechtel and abrahamsen (2005) say, “a mechanism is a structure performing a function in virtue of its component parts, component operations, and their organization. the orchestrated functioning of the mechanism is responsible for one or more phenomena.” (p. 423) in psychology, the adoption of the explanatory practice of identifying complex mechanisms in order to help explain psychological phenomena is more widespread than is copyrighted material; not for further distribution 27 generally realized. generically, the research strategy involves the decomposition of hierarchically organized systems into their components and operations and then building models in order to better understand the organization that comprises the mechanisms’ activities. the rise of the information-processing perspective in psychology explicitly adopted this mechanistic strategy at the level of the person, with the tote unit being a clear case in point. more recent examples from cognitive psychology that focus on sub-personal mechanisms can be found in wright and bechtel (2007). from data to mechanisms: a hybrid model the received view in the philosophy of biology depicts molecular biology as employing strategies and methods that facilitate the discovery of causal mechanisms (craver & darden, 2013). today, molecular biology is home to the most concerted debates about whether the advent of big data has ushered in a new data-driven view of science that replaces its traditional search for causal mechanisms (leonelli, 2016). one instructive reconstruction of some parts of contemporary biological research is offered by ratti (2015), who rejects the view that the two approaches represent separate lines of inquiry. instead, he presents a hybrid model that combines both data-driven and hypothesis-driven approaches. the idea is that, first, data-driven strategies are employed to generate hypotheses. then, hypotheses are developed and tested with methods that enable the discovery of causal mechanisms. this hybrid formulation of data-driven and hypothesis-driven research comprises three phases: (1) the initial formulation of a set of competing hypotheses; (2) the elimination of false, or less probable, hypotheses; and (3) the testing or validation of hypotheses that have not been eliminated in phase 2. taken together, the first two phases constitute a strategy involving eliminative inference in which implausible hypotheses are discarded. in addition to discarding copyrighted material; not for further distribution 28 hypotheses, phase 2 involves the prioritization of hypotheses that seem worth retaining, and submitting them to stronger testing in phase 3. a sophisticated view of the hypothetico-deductive method has a legitimate role to play in this third phase. it should be made clear that not all data-driven research fits this hybrid model of inquiry. for instance, data-mining studies, in the form of exploratory experiments, do not directly form part of the hybrid model, although they might provide guidelines for eliminative inference that is itself employed in the hybrid model (ratti, 2015). further, the data-driven hypothesis-driven sequence is just one way in which researchers attempt to get purchase on causal mechanisms. finally, this composite research strategy, with mechanistic causation as the focus of its third phase, can be understood more deeply by placing it within the broader framework of the philosophy of science known as the “new mechanical philosophy”, a philosophy which centers on historically informed scientific practices to do with discovery, modelling, and explanation (craver & darden, 2013). moreover, this philosophy might naturally be considered something of a local realist philosophy of science for big data science. some big data science enthusiasts claim that causation should be jettisoned, and that correlational information alone is sufficient for science (“correlation supersedes causation”). however, we have seen that some sophisticated philosophical analyses of big data science do, in fact, make explicit use of causal thinking, and thereby join with other modes of scientific research in claiming that causal inferences can be based on correlational data (“correlation implies causation”). in light of the fact that different accounts of causation have different positive features, i think that big data science should accommodate a plurality of conceptions of causation, including regularity and mechanistic theories, but also conceptions of causality that do not appeal to causal mechanisms, such as the difference-making account. copyrighted material; not for further distribution 29 conclusion in discussing big data science from a philosophy of science perspective, i have presented many different ideas. it might be useful therefore to bring together the main points in the form of a brief summary before offering a few closing words. 1. modern philosophy of science, in both its global and local forms, has considerable resources to help us better understand different conceptions of science, whether they deal with big or small data. this chapter has provided an overview of realist philosophy of science as a conceptual backdrop to the discussion of big data science, and has used it selectively in order to understand and evaluate a variety of claims made about it. 2. the three revolutionary claims, frequently mentioned in discussions of big data science, that it should eschew scientific method, explanatory theory, and causal knowledge, are implausible. all three endeavors remain vitally important for successful science. 3. the abductive theory of scientific method can provide a useful methodological perspective for big data science. further, the alternative perspectival accounts of scientific method of pietsch and ratti should also be understood as having decent, but different, philosophical justifications that can be used to support their adoption. 4. inductive, abductive, and hypothetio-deductive theories of scientific method can each be used to help meet their appropriate, and different, research goals in both big and small data science. inductive method plays an important role in the detection of empirical phenomena; different abductive methods combine to enable the construction of explanatory theories; and a modified hypothetico-deductive account of method can be used to test knowledge claims produced by the implementation of big data research strategies. 5. there is no canonical formulation of scientific theories. instead there are different copyrighted material; not for further distribution 30 conceptions of theories, expressed as networks of propositions, each of which have a legitimate claim to representing knowledge claims. the idea of classificatory theories is an important addition to our inventory of scientific theories that comes directly from the field of data-intensive biology. 6. a pluralist attitude to big data science that allows different conceptions of causation to operate is needed in order to contribute to the variety of causal claims made in scientific research. at a minimum, this will include the regularity, difference-making, and mechanistic theories. i bring this concluding section to an end with three briefly-stated thoughts. first, we have seen that big data science has a variety of defensible formulations. so, it is inappropriate to characterize it as a new paradigm, at least in kuhn’s (1970) sense of that idea. like the rest of science, it is multi-paradigmatic and, therefore, theoretically pluralist. second, the ability to fashion local realist construals of big data science allows us to understand “flattened”, seemingly empiricist, characterizations of big data science, as locally realist in character. for example, leonelli’s philosophical treatment of data-centric biology, with its sponsorship of classificatory theories, is nonetheless seen by her as consistent with hasok chang’s (2012) brand of scientific realism, a form of realism that endorses a strong form of methodological pluralism, and rejects the standard realist view that truth is paramount in science in favor of an emphasis on scientific progress. however, given pietsch’s reluctance to go beyond empirical phenomena, his take on data-intensive science is better seen a limited brand of local empiricism. finally, it is a significant truism that science, as we know it, is a human endeavor, and that human judgment is unavoidably involved in the production of its knowledge claims. at the copyrighted material; not for further distribution 31 same time, the emergence of big data science helps us to see that, as more science is carried out automatically through the use of electronic devices, parts of it can be seen as less of a human activity (lyon, 2016). this trend can be viewed as one aspect of the evolutionary march of science for “limited beings”, a conception of science for which william wimsatt’s (2007) local realist philosophy is explicitly devised. however, it should be appreciated that this is a perspectival realism, in the sense that science cannot rise above our human cognitive framework. werner callebaut (2012) also thinks that perspectival realism is the most appropriate philosophy for big data biological science. for both philosophers, perspectival realism has the value of emphasizing the complex, pluralist, and pragmatist nature of science. future work on the conceptual foundations of big data science would do well to seriously consider this important formulation of realism. references anderson, c. (2008). the end of theory: the data deluge makes the scientific method obsolete. wired magazine 16.07 (23 june, 2008). retrieved from http://www.wired.com/science/discoveries/magazine/16-07/pb_theory. bechtel, w., & abrahamsen, a. (2005). explanation: a mechanist alternative. studies in history and philosophy of biological and biomedical sciences, 36, 421-441. bunge, m. (1990). what kind of discipline is psychology: autonomous or dependent, humanistic or scientific, biological or sociological? new ideas in psychology, 8, 121-137. callebaut, w. (2012). scientific perspectivism: a philosopher of science’s response to the challenge of big data biology. studies in the history and philosophy of biological and biomedical sciences, 43, 69-80. chalmers, a. f. (2013). what is this thing called science? (4th ed.). st lucia, australia: copyrighted material; not for further distribution 32 university of queensland press. chang, h. (2012). is water h2o? evidence, realism, and pluralism. dordrecht, the netherlands: springer. craver, c. f., & darden, l. (2013). in search of mechanisms. chicago, il: university of chicago press. cronbach, l. j., & meehl, p. e. (1955). construct validity in psychological tests. psychological bulletin, 52, 281-302. curd, m. (1980). the logic of discovery: an analysis of three approaches. in t. nickles (ed.), scientific discovery, logic, and rationality (pp. 201-219). dordrecht, the netherlands: reidel. fox, p., & hendler, j. (2014). the science of data science. big data, 2, 68-70. glennan, s. (1996). mechanisms and the nature of causation. erkenntnis, 44, 50-71. godfrey-smith, p. (2009). causal pluralism. in h. beebee, p. menzies, & c. hitchcock (eds.), the oxford handbook of causation (pp. 326-337). new york, n.y.: oxford university press. haig, b. d. (2003). what is a spurious correlation? understanding statistics, 2, 125-132. haig, b. d. (2005). an abductive theory of scientific method. psychological methods, 10, 371388. haig, b. d. (2014). investigating the psychological world: scientific method in the behavioral sciences. cambridge, ma: mit press. harlow, l. l., & oswald, f. l. (2016). big data in psychology: an introduction to the special issue. psychological methods, 21, 447-457. harré, r., & madden, e. h. (1975). causal powers: a theory of natural necessity. oxford, copyrighted material; not for further distribution 33 england: basil blackwell. hey, a. j. g., tansley, s., & tolle, k. (eds.) (2009). jim gray on escience: a transformed scientific method. in hey, a. j. g., tansley, s., & tolle, k. (eds.). the fourth paradigm: data-intensive scientific discovery (pp. xvii-xxxi). redmond, wa: microsoft research hooker, c. a. (1987). a realistic theory of science. albany, ny: state university of new york press. johns, b. t., jamieson, r. k., & jones, m. n. (2018). the continued importance of theory: lessons from big data approaches to cognition. in woo, s. e., proctor, r., & l. tay (eds.), big data in psychological sciences (in press). washington, d. c.: apa books. kenny, d. (1979). correlation and causation. new york, ny: wiley. kincaid, h. (2000). global arguments and local realism about the social sciences. philosophy of science (supplement), 67, 667-678. kitchin, r. (2014). big data, new epistemologies, and paradigm shifts. big data & society, 1, 112. kuhn, t. s. (1970). the structure of scientific revolutions (2nd ed.). chicago, il: university of chicago press. leonelli, s. (2016). data-centric biology: a philosophical study. chicago, il: university of chicago press. locke, e. a. (2015). theory building, replication, and behavioral priming: where do we need to go from here? perspectives on psychological science, 10, 408-414. lyon, a. (2016). data. in p. humphreys (ed.), the oxford handbook of philosophy of science (pp. 738-758). new york, ny: oxford university press. magnani, l. (2001). abduction, reason, and science: processes of discovery and explanation. copyrighted material; not for further distribution 34 new york, ny: kluwer/plenum. mäki, u. (2005). reglobalizing realism by going local, or (how) should our formulation of realism be informed about the sciences? erkenntnis, 63, 231-251. mayer-schonberger, v., & cukier, k. (2013). big data: a revolution that will transform how we live, work, and think. boston, ma: houghton mifflin harcourt. mcabee, s. t., landis, r. s., & burke, m. i. (2017). inductive reasoning: the promise of big data. human resource management review, 27, 277-290. nickles, t. (1987). methodology, heuristics, and rationality. in j. c. pitt & m. pera (eds.), rational changes in science (pp. 103-132). dordrecht, the netherlands: reidel. oswald, f. l., & putka, d. j. (2016). statistical methods for big data: a scenic tour. in e. b king & j. m. cortina (eds.), big data at work: the data science revolution and organizational psychology (pp. 43-63). new york, ny: routledge. pearson, k. (1911). the grammar of science (3rd ed.). london, england: adam & charles black. pietsch, w. (2015). aspects of theory-ladenness in data-intensive science. philosophy of science, 82, 905-916. pietsch, w. (2016). the causal nature of modeling with big data. philosophy & technology, 29, 137-171. pietch, w. (2018). big data: the new science of complexity. (unpublished manuscript). psillos, s. (1999). scientific realism: how science tracks the truth. london, england: routledge. ratti, e. (2015). big data biology: between eliminative inferences and exploratory experiments. philosophy of science, 82, 198-218. simon, h. a. (1956). rational choice and the structure of the environment. psychological review, 63, 129-138. copyrighted material; not for further distribution 35 skinner, b. f. (1984). methods and theories in the experimental analysis of behavior. behavioral and brain sciences, 7, 511-546 thagard, p. (1992). conceptual revolutions. princeton, nj: princeton university press. tonidandel, s., king, e. b., & cortina, j. m. (2018). big data methods: leveraging modern data analytic techniques to build organizational science. organizational research methods, 21, 525-547. wimsatt, w. (2007). re-engineering philosophy for limited beings: piecewise approximations to reality. cambridge, ma: harvard university press. wright, c., & bechtel, w. (2007). mechanisms and psychological explanation. in p. thagard (ed.), handbook of the philosophy of science: philosophy of psychology and cognitive science (pp. 31-79). amsterdam, the netherlands: elsevier. copyrighted material; not for further distribution